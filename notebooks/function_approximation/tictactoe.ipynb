{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tonywy/micromamba/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cpu\n",
            "dqn_agent test passed!\n",
            "using device: cpu\n",
            "policy_gradient_agent_test passed!\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Setup Environment\n",
        "import sys\n",
        "import os\n",
        "ROOT = '../../'\n",
        "sys.path.append(ROOT)\n",
        "\n",
        "from pettingzoo.classic import tictactoe_v3\n",
        "import numpy as np\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "from IPython.display import HTML\n",
        "from source.agents.dqn_agent import DQNAgent\n",
        "from source.agents.policy_gradient_with_baseline_agent import PolicyGradientWithBaselineAgent\n",
        "from source.agents.random_agent import RandomAgent\n",
        "from source.utils import utils\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import random\n",
        "from typing import Dict, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "from source.agents.agent import Agent\n",
        "from pettingzoo.utils.env import AECEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cpu\n",
            "using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:17<00:00, 58.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 0, epsilon: 0.05, average_return: 0.5901803607214429, success rate: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:27<00:00, 36.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 1, epsilon: 0.05, average_return: 0.3527054108216433, success rate: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:27<00:00, 36.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 2, epsilon: 0.05, average_return: 0.3076152304609218, success rate: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:27<00:00, 36.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 3, epsilon: 0.05, average_return: 0.2575150300601202, success rate: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 392/1000 [00:11<00:16, 36.52it/s]"
          ]
        }
      ],
      "source": [
        "# Policy Eval\n",
        "# Params\n",
        "num_epoch = 5\n",
        "num_episode = 1000\n",
        "video_path = os.path.join(ROOT, \"video/dqn_tictactoe.mp4\")\n",
        "random_seed = 101\n",
        "\n",
        "# Initialize\n",
        "history = []\n",
        "total_reward = 0\n",
        "# set random seeds\n",
        "rng = np.random.default_rng(random_seed)\n",
        "#random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "#torch.manual_seed(random_seed)\n",
        "# epsilon decay\n",
        "epsilon_schedule = utils.create_decay_schedule(num_epoch)\n",
        "lr_schedule = utils.create_decay_schedule(num_epoch)\n",
        "\n",
        "# Create Environment.\n",
        "#env = gym.make('Taxi-v3')\n",
        "# using render_mode=rgb_array so that video recording works\n",
        "#env = gym.make(\n",
        "#    \"LunarLander-v2\",\n",
        "#    render_mode='rgb_array'\n",
        "#)\n",
        "#env = gym.make(\"CarRacing-v2\", render_mode='rgb_array', continuous=False)\n",
        "#env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "\n",
        "env = tictactoe_v3.env()\n",
        "env.np_random = rng\n",
        "\n",
        "# Create Agent\n",
        "dqn_agent = DQNAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.99,\n",
        "    epsilon=0.9, # use epsilon_schedule\n",
        "    learning_rate=1e-5,\n",
        "    learning=True,\n",
        "    batch_size = 1000,\n",
        "    tau = 0.005,\n",
        "    eps_decay=3000,\n",
        "    net_params={'width':16, 'n_hidden':2}\n",
        ")\n",
        "pg_agent = PolicyGradientWithBaselineAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.99,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    policy_lr=1e-3,\n",
        "    value_lr=1e-3,\n",
        "    net_params={'width':32, 'n_hidden':1}\n",
        ")\n",
        "random_agent = RandomAgent(    \n",
        "    state_space=env.observation_space('player_2')['observation'],\n",
        "    action_space=env.action_space('player_2'),\n",
        "    discount_rate=None,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    learning=False\n",
        ")\n",
        "agent_dict = {'player_1': dqn_agent, 'player_2':random_agent}\n",
        "eps_history = []\n",
        "# Start Learning\n",
        "for i in range(num_epoch):\n",
        "    success = 0\n",
        "    for _ in tqdm(range(num_episode)):\n",
        "        reward, steps = utils.play_multiagent_episode(agent_dict, env)#,epsilon=epsilon_schedule[i])\n",
        "        history.append(reward['player_1'])\n",
        "        eps_history.append(agent_dict['player_1']._epsilon)\n",
        "        total_reward += reward['player_1']\n",
        "    #score = agent.update()\n",
        "    print(\n",
        "        f\"step: {i}, epsilon: {dqn_agent._epsilon}, average_return: {np.mean(history[-num_episode+1:-1])}, success rate: {success / num_episode}\")\n",
        "print(f\"\\nrewarding episodes: {total_reward}\")\n",
        "\n",
        "# For off policy learning only: get greedy policy (no exploration)\n",
        "#agent._policy = get_epsilon_greedy_policy_from_action_values(agent._Q.weight)\n",
        "# Run Eval\n",
        "avarge_return, num_episode = utils.evaluate_multiagent(agent_dict, env, 5000)\n",
        "for ag_id, value in avarge_return.items():\n",
        "    print(f\"{ag_id}: Average return = {value / num_episode}\")\n",
        "\n",
        "utils.plot_history(history)\n",
        "utils.plot_history(eps_history, smoothing=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  O  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  -  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  O  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  O  |  -  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  O  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  O  |  -  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  X  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  O  |  O  \n",
            "     |     |     \n"
          ]
        }
      ],
      "source": [
        "env = tictactoe_v3.env(render_mode='human')\n",
        "reward, _ = utils.play_multiagent_episode(agent_dict, env)\n",
        "#html=utils.render_mp4(video_path)\n",
        "#HTML(html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b007964239c9846de49217bea874a76b6e18c6041f326c6a02623c321aae0990"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tonywy/micromamba/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cpu\n",
            "dqn_agent test passed!\n",
            "using device: cpu\n",
            "policy_gradient_agent_test passed!\n",
            "using device: cpu\n",
            "ppo_agent passed!\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Setup Environment\n",
        "import sys\n",
        "import os\n",
        "ROOT = '../../'\n",
        "sys.path.append(ROOT)\n",
        "\n",
        "from pettingzoo.classic import tictactoe_v3\n",
        "import numpy as np\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "from IPython.display import HTML\n",
        "from source.agents.dqn_agent import DQNAgent\n",
        "from source.agents.policy_gradient_with_baseline_agent import PolicyGradientWithBaselineAgent\n",
        "from source.agents.ppo_agent import PPOAgent\n",
        "from source.agents.random_agent import RandomAgent\n",
        "from source.utils import utils\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import random\n",
        "from typing import Dict, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "from source.agents.agent import Agent\n",
        "from pettingzoo.utils.env import AECEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cpu\n",
            "using device: cpu\n",
            "using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 130.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 0, epsilon: 0.9, average_return: 0.5591182364729459, success rate: 0.744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 132.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 1, epsilon: 0.9, average_return: 0.843687374749499, success rate: 0.912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 131.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 2, epsilon: 0.9, average_return: 0.5390781563126252, success rate: 0.719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 129.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 3, epsilon: 0.9, average_return: 0.5390781563126252, success rate: 0.725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 872/1000 [00:06<00:00, 128.10it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [2], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m success \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_episode)):\n\u001b[0;32m---> 85\u001b[0m     reward, wins, steps \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mplay_multiagent_episode(agent_dict, env)\u001b[39m#,epsilon=epsilon_schedule[i])\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     history\u001b[39m.\u001b[39mappend(reward[\u001b[39m'\u001b[39m\u001b[39mplayer_1\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     87\u001b[0m     \u001b[39m#eps_history.append(agent_dict['player_1']._epsilon)\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/utils/utils.py:256\u001b[0m, in \u001b[0;36mplay_multiagent_episode\u001b[0;34m(agent_dict, env)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[39m# if steps > 1000:\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m#    terminal = True\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m agent\u001b[39m.\u001b[39m_learning:\n\u001b[0;32m--> 256\u001b[0m     agent\u001b[39m.\u001b[39;49mcontrol()\n\u001b[1;32m    257\u001b[0m \u001b[39mreturn\u001b[39;00m total_reward, wins, steps\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/agents/ppo_agent.py:123\u001b[0m, in \u001b[0;36mPPOAgent.control\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcontrol\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_updates):\n\u001b[0;32m--> 123\u001b[0m         advantage_tensor, importance_tensor, mean_approx_kl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_batch()\n\u001b[1;32m    124\u001b[0m         \u001b[39m# Value Update.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         value_loss_tensor \u001b[39m=\u001b[39m (advantage_tensor \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/agents/ppo_agent.py:96\u001b[0m, in \u001b[0;36mPPOAgent.process_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m state_tensor, action_tensor, next_state_tensor, reward, terminal \u001b[39m=\u001b[39m trans\n\u001b[1;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 96\u001b[0m     next_state_value_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_value_net(next_state_tensor)\n\u001b[1;32m     97\u001b[0m state_value_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_net(state_tensor)\n\u001b[1;32m     98\u001b[0m delta \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_discount_rate \u001b[39m*\u001b[39m \\\n\u001b[1;32m     99\u001b[0m     next_state_value_tensor \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m terminal) \u001b[39m-\u001b[39m state_value_tensor\n",
            "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/net.py:20\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layers:\n\u001b[1;32m     19\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(layer(x))\n\u001b[0;32m---> 20\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer(x)\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_softmax:\n\u001b[1;32m     22\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(x, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
            "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1260\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m   1259\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m _parameters:\n\u001b[0;32m-> 1260\u001b[0m         \u001b[39mreturn\u001b[39;00m _parameters[name]\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_buffers\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1262\u001b[0m     _buffers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_buffers\u001b[39m\u001b[39m'\u001b[39m]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Policy Eval\n",
        "# Params\n",
        "num_epoch = 20\n",
        "num_episode = 1000\n",
        "video_path = os.path.join(ROOT, \"video/dqn_tictactoe.mp4\")\n",
        "random_seed = 101\n",
        "\n",
        "# Initialize\n",
        "history = []\n",
        "total_reward = 0\n",
        "total_wins = 0\n",
        "# set random seeds\n",
        "rng = np.random.default_rng(random_seed)\n",
        "#random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "#torch.manual_seed(random_seed)\n",
        "# epsilon decay\n",
        "epsilon_schedule = utils.create_decay_schedule(num_epoch)\n",
        "lr_schedule = utils.create_decay_schedule(num_epoch)\n",
        "\n",
        "# Create Environment.\n",
        "#env = gym.make('Taxi-v3')\n",
        "# using render_mode=rgb_array so that video recording works\n",
        "#env = gym.make(\n",
        "#    \"LunarLander-v2\",\n",
        "#    render_mode='rgb_array'\n",
        "#)\n",
        "#env = gym.make(\"CarRacing-v2\", render_mode='rgb_array', continuous=False)\n",
        "#env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "\n",
        "env = tictactoe_v3.env()\n",
        "env.np_random = rng\n",
        "\n",
        "# Create Agent\n",
        "dqn_agent = DQNAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.99,\n",
        "    epsilon=0.9, # use epsilon_schedule\n",
        "    learning_rate=1e-3,\n",
        "    learning=True,\n",
        "    batch_size = 16,\n",
        "    tau = 0.005,\n",
        "    eps_decay=3000,\n",
        "    net_params={'width':64, 'n_hidden':2},\n",
        "    update_freq=100\n",
        ")\n",
        "pg_agent = PolicyGradientWithBaselineAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.99,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    policy_lr=1e-3,\n",
        "    value_lr=1e-3,\n",
        "    net_params={'width':32, 'n_hidden':1}\n",
        ")\n",
        "ppo_agent = PPOAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.99,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    policy_lr=1e-4,\n",
        "    value_lr=1e-4,\n",
        "    net_params={'width':32, 'n_hidden':1},\n",
        "    exp_average_discount=1.0,\n",
        "    policy_shift_tol=0.1,\n",
        "    num_updates=5,\n",
        ")\n",
        "random_agent = RandomAgent(    \n",
        "    state_space=env.observation_space('player_2')['observation'],\n",
        "    action_space=env.action_space('player_2'),\n",
        "    discount_rate=None,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    learning=False\n",
        ")\n",
        "agent_dict = {'player_1': ppo_agent, 'player_2':random_agent}\n",
        "eps_history = []\n",
        "# Start Learning\n",
        "for i in range(num_epoch):\n",
        "    success = 0\n",
        "    for _ in tqdm(range(num_episode)):\n",
        "        reward, wins, steps = utils.play_multiagent_episode(agent_dict, env)#,epsilon=epsilon_schedule[i])\n",
        "        history.append(reward['player_1'])\n",
        "        #eps_history.append(agent_dict['player_1']._epsilon)\n",
        "        total_reward += reward['player_1']\n",
        "        success += wins['player_1']\n",
        "    #score = agent.update()\n",
        "    print(\n",
        "        f\"step: {i}, epsilon: {dqn_agent._epsilon}, average_return: {np.mean(history[-num_episode+1:-1])}, success rate: {success / num_episode}\")\n",
        "print(f\"\\nrewarding episodes: {total_reward}\")\n",
        "\n",
        "# For off policy learning only: get greedy policy (no exploration)\n",
        "#agent._policy = get_epsilon_greedy_policy_from_action_values(agent._Q.weight)\n",
        "# Run Eval\n",
        "avarge_return, num_episode = utils.evaluate_multiagent(agent_dict, env, 5000)\n",
        "for ag_id, value in avarge_return.items():\n",
        "    print(f\"{ag_id}: Average return = {value / num_episode}\")\n",
        "\n",
        "utils.plot_history(history)\n",
        "#utils.plot_history(eps_history, smoothing=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  X  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  X  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  -  |  -  \n",
            "     |     |     \n"
          ]
        }
      ],
      "source": [
        "env = tictactoe_v3.env(render_mode='human')\n",
        "reward, _ = utils.play_multiagent_episode(agent_dict, env)\n",
        "#html=utils.render_mp4(video_path)\n",
        "#HTML(html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b007964239c9846de49217bea874a76b6e18c6041f326c6a02623c321aae0990"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Setup Environment\n",
        "import sys\n",
        "import os\n",
        "ROOT = '../../'\n",
        "sys.path.append(ROOT)\n",
        "\n",
        "from pettingzoo.classic import tictactoe_v3\n",
        "import numpy as np\n",
        "import copy\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "from IPython.display import HTML\n",
        "from source.agents.dqn_agent import DQNAgent\n",
        "from source.agents.advantage_actor_critic_agent import A2CAgent\n",
        "from source.agents.ppo_agent import PPOAgent\n",
        "from source.agents.random_agent import RandomAgent\n",
        "from source.utils import utils\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import random\n",
        "from typing import Dict, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "from source.agents.agent import Agent\n",
        "from pettingzoo.utils.env import AECEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cpu\n",
            "using device: cpu\n",
            "using device: cpu\n",
            "agents: {'player_1': <source.agents.ppo_agent.PPOAgent object at 0x14fb24c40>, 'player_2': <source.agents.ppo_agent.PPOAgent object at 0x2a0bb50a0>}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 357.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, win: 427, lose: 458, draw: 115, reward: -0.03100, episode_len: 4.72800, value_loss: 0.13188, policy_loss: -0.05835, num_policy_udpate: 80.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:03<00:00, 300.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, win: 441, lose: 475, draw: 84, reward: -0.03400, episode_len: 4.51900, value_loss: 0.11593, policy_loss: -0.06077, num_policy_udpate: 80.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 377.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 2, win: 491, lose: 449, draw: 60, reward: 0.04200, episode_len: 4.42300, value_loss: 0.11210, policy_loss: -0.00325, num_policy_udpate: 4.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 477.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 3, win: 493, lose: 450, draw: 57, reward: 0.04300, episode_len: 4.29900, value_loss: 0.09892, policy_loss: -0.04062, num_policy_udpate: 32.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 463.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 4, win: 530, lose: 414, draw: 56, reward: 0.11600, episode_len: 4.21100, value_loss: 0.10834, policy_loss: -0.01278, num_policy_udpate: 4.66667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 560.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 5, win: 448, lose: 488, draw: 64, reward: -0.04000, episode_len: 4.10200, value_loss: 0.10187, policy_loss: -0.01772, num_policy_udpate: 3.50000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 565.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 6, win: 478, lose: 441, draw: 81, reward: 0.03700, episode_len: 4.15400, value_loss: 0.09740, policy_loss: -0.02360, num_policy_udpate: 5.66667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 569.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 7, win: 424, lose: 471, draw: 105, reward: -0.04700, episode_len: 4.11600, value_loss: 0.07927, policy_loss: -0.01618, num_policy_udpate: 4.33333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 521.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 8, win: 461, lose: 366, draw: 173, reward: 0.09500, episode_len: 4.30900, value_loss: 0.08343, policy_loss: -0.01587, num_policy_udpate: 27.33333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 431.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9, win: 368, lose: 373, draw: 259, reward: -0.00500, episode_len: 4.42500, value_loss: 0.06387, policy_loss: -0.03687, num_policy_udpate: 30.75000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 439.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 10, win: 341, lose: 323, draw: 336, reward: 0.01800, episode_len: 4.55500, value_loss: 0.05335, policy_loss: -0.03596, num_policy_udpate: 32.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 429.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 11, win: 311, lose: 259, draw: 430, reward: 0.05200, episode_len: 4.66300, value_loss: 0.04639, policy_loss: -0.01034, num_policy_udpate: 15.33333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 142/1000 [00:00<00:02, 397.53it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [20], line 74\u001b[0m\n\u001b[1;32m     52\u001b[0m ppo_agent \u001b[39m=\u001b[39m PPOAgent(\n\u001b[1;32m     53\u001b[0m     state_space\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mobservation_space(\u001b[39m'\u001b[39m\u001b[39mplayer_1\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     54\u001b[0m     action_space\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space(\u001b[39m'\u001b[39m\u001b[39mplayer_1\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     66\u001b[0m random_agent \u001b[39m=\u001b[39m RandomAgent(    \n\u001b[1;32m     67\u001b[0m     state_space\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mobservation_space(\u001b[39m'\u001b[39m\u001b[39mplayer_2\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     68\u001b[0m     action_space\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space(\u001b[39m'\u001b[39m\u001b[39mplayer_2\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     learning\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m stats \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mduel_training(\n\u001b[1;32m     75\u001b[0m     env\u001b[39m=\u001b[39;49menv, \n\u001b[1;32m     76\u001b[0m     agent_dict\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mplayer_1\u001b[39;49m\u001b[39m'\u001b[39;49m: ppo_agent, \u001b[39m'\u001b[39;49m\u001b[39mplayer_2\u001b[39;49m\u001b[39m'\u001b[39;49m: dqn_agent}, \n\u001b[1;32m     77\u001b[0m     num_epoch\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \n\u001b[1;32m     78\u001b[0m     num_episode\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, \n\u001b[1;32m     79\u001b[0m     self_play\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m     80\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m     81\u001b[0m     verbal\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     82\u001b[0m     debug\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m )\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/utils/utils.py:316\u001b[0m, in \u001b[0;36mduel_training\u001b[0;34m(env, agent_dict, num_epoch, num_episode, self_play, shuffle, verbal, debug)\u001b[0m\n\u001b[1;32m    314\u001b[0m logging \u001b[39m=\u001b[39m defaultdict(\u001b[39mlambda\u001b[39;00m: defaultdict(\u001b[39mlist\u001b[39m))\n\u001b[1;32m    315\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_episode)):\n\u001b[0;32m--> 316\u001b[0m     logs \u001b[39m=\u001b[39m play_multiagent_episode(\n\u001b[1;32m    317\u001b[0m         agent_dict, env, shuffle\u001b[39m=\u001b[39;49mshuffle, debug\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    318\u001b[0m     \u001b[39mfor\u001b[39;00m agent_id, log \u001b[39min\u001b[39;00m logs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    319\u001b[0m         \u001b[39mfor\u001b[39;00m name, metric \u001b[39min\u001b[39;00m log\u001b[39m.\u001b[39mitems():\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/utils/utils.py:289\u001b[0m, in \u001b[0;36mplay_multiagent_episode\u001b[0;34m(agent_dict, env, shuffle, debug)\u001b[0m\n\u001b[1;32m    287\u001b[0m     logs[shuffled[agent_id]][\u001b[39m'\u001b[39m\u001b[39mepisode_len\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[39mfor\u001b[39;00m agent_id, agent \u001b[39min\u001b[39;00m agent_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 289\u001b[0m     loss_dict \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mcontrol()\n\u001b[1;32m    290\u001b[0m     \u001b[39mif\u001b[39;00m loss_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(loss_dict) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    291\u001b[0m         logs[shuffled[agent_id]] \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m loss_dict\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/agents/ppo_agent.py:247\u001b[0m, in \u001b[0;36mPPOAgent.control\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m# Policy Update\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m#policy_loss = torch.tensor(0)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_updates):\n\u001b[0;32m--> 247\u001b[0m     policy_loss, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_policy_loss(data)\n\u001b[1;32m    248\u001b[0m     \u001b[39m# If KL divergence is too large, the new policy is diverging from old policy, stop training since it could lead to unstable/bad udpates.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39mapprox_kl\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_kl:\n\u001b[1;32m    250\u001b[0m         \u001b[39m#print(f'Early stopping at step {_} due to {mean_approx_kl} reaching max kl.')\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/agents/ppo_agent.py:203\u001b[0m, in \u001b[0;36mPPOAgent.compute_policy_loss\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_policy_loss\u001b[39m(\u001b[39mself\u001b[39m, data: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    200\u001b[0m     \u001b[39m# data is Batched dataset: [batch_size, ...]\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     obs, act, adv, logp_old \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mact\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39madv\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mlogp\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 203\u001b[0m     p_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_net(obs) \n\u001b[1;32m    204\u001b[0m     dist \u001b[39m=\u001b[39m Categorical(p_actions)\n\u001b[1;32m    205\u001b[0m     logp \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39mlog_prob(act)\n",
            "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/net.py:22\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layers:\n\u001b[1;32m     21\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(layer(x))\n\u001b[0;32m---> 22\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer(x)\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_softmax:\n\u001b[1;32m     24\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(x \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tempreture, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Policy Eval\n",
        "# Params\n",
        "env = tictactoe_v3.env()\n",
        "\n",
        "# set random seeds\n",
        "random_seed = 101\n",
        "rng = np.random.default_rng(random_seed)\n",
        "env.np_random = rng\n",
        "#random.seed(random_seed)\n",
        "#torch.manual_seed(random_seed)\n",
        "\n",
        "# Create Environment.\n",
        "#env = gym.make('Taxi-v3')\n",
        "# using render_mode=rgb_array so that video recording works\n",
        "#env = gym.make(\n",
        "#    \"LunarLander-v2\",\n",
        "#    render_mode='rgb_array'\n",
        "#)\n",
        "#env = gym.make(\"CarRacing-v2\", render_mode='rgb_array', continuous=False)\n",
        "#env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "\n",
        "\n",
        "# Create Agent\n",
        "## Best performance (dqn, random): 0 loss in 1000, eps_decay=1e7\n",
        "## Best performance (dqn, dqn): 1000 draw in 1000, eps_decay=1e8\n",
        "dqn_agent = DQNAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.5,\n",
        "    epsilon=0.9, # use epsilon_schedule\n",
        "    learning_rate=1e-3,\n",
        "    learning=True,\n",
        "    batch_size = 64,\n",
        "    tau = 0.005,\n",
        "    eps_decay=1e7, #1e7 for (dqn, random), 1e8 for (dqn, dqn)\n",
        "    net_params=[128],\n",
        "    update_freq=1\n",
        ")\n",
        "## Best performance (a2c, random): ~50 loss in 1000, discount=0.5, width=256\n",
        "## \n",
        "a2c_agent = A2CAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.5,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    policy_lr=1e-3,\n",
        "    value_lr=1e-3,\n",
        "    net_params=[256], #128 perform best with temp = 1\n",
        "    tempreture=1\n",
        ")\n",
        "ppo_agent = PPOAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.5,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    policy_lr=1e-3, #spinup default=3e-4\n",
        "    value_lr=1e-3, #spinup default=1e-3\n",
        "    net_params=[128, 16],\n",
        "    gae_lambda=0.97, #spinup default=0.97\n",
        "    clip_ratio=0.1, #spinup default=0.1\n",
        "    num_updates=80, #spinup default=80\n",
        "    batch_size=1000,\n",
        ")\n",
        "random_agent = RandomAgent(    \n",
        "    state_space=env.observation_space('player_2')['observation'],\n",
        "    action_space=env.action_space('player_2'),\n",
        "    discount_rate=None,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    learning=False\n",
        ")\n",
        "stats = utils.duel_training(\n",
        "    env=env, \n",
        "    agent_dict={'player_1': ppo_agent, 'player_2': dqn_agent}, \n",
        "    num_epoch=100, \n",
        "    num_episode=1000, \n",
        "    self_play=False, \n",
        "    shuffle=True, \n",
        "    verbal=True,\n",
        "    debug=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  -  |  O  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  -  |  O  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  -  |  O  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  X  |  -  |  O  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  X  |  O  |  O  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  X  |  O  |  O  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  X  |  O  \n",
            "     |     |     \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "defaultdict(<function source.utils.utils.play_multiagent_episode.<locals>.<lambda>()>,\n",
              "            {'player_1': defaultdict(float,\n",
              "                         {'reward': 0.0, 'episode_len': 6.0}),\n",
              "             'player_2': defaultdict(float,\n",
              "                         {'reward': 0.0, 'episode_len': 5.0})})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = tictactoe_v3.env(render_mode='human')\n",
        "utils.play_multiagent_episode({'player_1': ppo_agent, 'player_2': ppo_agent}, env, shuffle=False,debug=False)\n",
        "#html=utils.render_mp4(video_path)\n",
        "#HTML(html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b007964239c9846de49217bea874a76b6e18c6041f326c6a02623c321aae0990"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Setup Environment\n",
        "import sys\n",
        "import os\n",
        "ROOT = '../../'\n",
        "sys.path.append(ROOT)\n",
        "\n",
        "from pettingzoo.classic import tictactoe_v3\n",
        "import numpy as np\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "from IPython.display import HTML\n",
        "from source.agents.dqn_agent import DQNAgent\n",
        "from source.agents.policy_gradient_with_baseline_agent import PolicyGradientWithBaselineAgent\n",
        "from source.agents.random_agent import RandomAgent\n",
        "from source.utils import utils\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import random\n",
        "from typing import Dict, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "from source.agents.agent import Agent\n",
        "from pettingzoo.utils.env import AECEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cpu\n",
            "using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:07<00:00, 14.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 0, epsilon: 0.050000000000010265, average_return: 0.23469387755102042, success rate: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00, 12.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 1, epsilon: 0.05000000000000001, average_return: 0.2857142857142857, success rate: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:07<00:00, 14.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 2, epsilon: 0.05000000000000001, average_return: 0.16326530612244897, success rate: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:07<00:00, 13.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 3, epsilon: 0.05000000000000001, average_return: 0.22448979591836735, success rate: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 32/100 [00:02<00:04, 15.12it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [9], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m success \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_episode)):\n\u001b[0;32m---> 71\u001b[0m     reward, steps \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mplay_multiagent_episode(agent_dict, env)\u001b[39m#,epsilon=epsilon_schedule[i])\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     history\u001b[39m.\u001b[39mappend(reward[\u001b[39m'\u001b[39m\u001b[39mplayer_1\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     73\u001b[0m     eps_history\u001b[39m.\u001b[39mappend(agent_dict[\u001b[39m'\u001b[39m\u001b[39mplayer_1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39m_epsilon)\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/utils/utils.py:220\u001b[0m, in \u001b[0;36mplay_multiagent_episode\u001b[0;34m(agent_dict, env)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# if steps > 1000:\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#    terminal = True\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m agent\u001b[39m.\u001b[39m_learning:\n\u001b[0;32m--> 220\u001b[0m     agent\u001b[39m.\u001b[39;49mcontrol()\n\u001b[1;32m    221\u001b[0m \u001b[39mreturn\u001b[39;00m total_reward, steps\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/agents/dqn_agent.py:190\u001b[0m, in \u001b[0;36mDQNAgent.control\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcontrol\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    189\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_freq):\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimize_model()\n\u001b[1;32m    191\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_target_net()\n",
            "File \u001b[0;32m~/Desktop/mini-rl/source/agents/dqn_agent.py:159\u001b[0m, in \u001b[0;36mDQNAgent._optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39m# In-place gradient clipping\u001b[39;00m\n\u001b[1;32m    158\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_value_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy_net\u001b[39m.\u001b[39mparameters(), \u001b[39m100\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer\u001b[39m.\u001b[39;49mstep()\n",
            "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/optim/optimizer.py:138\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    137\u001b[0m     obj, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m args\n\u001b[0;32m--> 138\u001b[0m     profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39mOptimizer.step#\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.step\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(obj\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m    139\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m    140\u001b[0m         out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Policy Eval\n",
        "# Params\n",
        "num_epoch = 50\n",
        "num_episode = 100\n",
        "video_path = os.path.join(ROOT, \"video/dqn_tictactoe.mp4\")\n",
        "random_seed = 101\n",
        "\n",
        "# Initialize\n",
        "history = []\n",
        "total_reward = 0\n",
        "# set random seeds\n",
        "rng = np.random.default_rng(random_seed)\n",
        "#random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "#torch.manual_seed(random_seed)\n",
        "# epsilon decay\n",
        "epsilon_schedule = utils.create_decay_schedule(num_epoch)\n",
        "lr_schedule = utils.create_decay_schedule(num_epoch)\n",
        "\n",
        "# Create Environment.\n",
        "#env = gym.make('Taxi-v3')\n",
        "# using render_mode=rgb_array so that video recording works\n",
        "#env = gym.make(\n",
        "#    \"LunarLander-v2\",\n",
        "#    render_mode='rgb_array'\n",
        "#)\n",
        "#env = gym.make(\"CarRacing-v2\", render_mode='rgb_array', continuous=False)\n",
        "#env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "\n",
        "env = tictactoe_v3.env()\n",
        "env.np_random = rng\n",
        "\n",
        "# Create Agent\n",
        "dqn_agent = DQNAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.99,\n",
        "    epsilon=0.9, # use epsilon_schedule\n",
        "    learning_rate=1e-3,\n",
        "    learning=True,\n",
        "    batch_size = 16,\n",
        "    tau = 0.005,\n",
        "    eps_decay=3000,\n",
        "    net_params={'width':64, 'n_hidden':2},\n",
        "    update_freq=100\n",
        ")\n",
        "pg_agent = PolicyGradientWithBaselineAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.99,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    policy_lr=1e-3,\n",
        "    value_lr=1e-3,\n",
        "    net_params={'width':32, 'n_hidden':1}\n",
        ")\n",
        "random_agent = RandomAgent(    \n",
        "    state_space=env.observation_space('player_2')['observation'],\n",
        "    action_space=env.action_space('player_2'),\n",
        "    discount_rate=None,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    learning=False\n",
        ")\n",
        "agent_dict = {'player_1': dqn_agent, 'player_2':dqn_agent}\n",
        "eps_history = []\n",
        "# Start Learning\n",
        "for i in range(num_epoch):\n",
        "    success = 0\n",
        "    for _ in tqdm(range(num_episode)):\n",
        "        reward, steps = utils.play_multiagent_episode(agent_dict, env)#,epsilon=epsilon_schedule[i])\n",
        "        history.append(reward['player_1'])\n",
        "        eps_history.append(agent_dict['player_1']._epsilon)\n",
        "        total_reward += reward['player_1']\n",
        "    #score = agent.update()\n",
        "    print(\n",
        "        f\"step: {i}, epsilon: {dqn_agent._epsilon}, average_return: {np.mean(history[-num_episode+1:-1])}, success rate: {success / num_episode}\")\n",
        "print(f\"\\nrewarding episodes: {total_reward}\")\n",
        "\n",
        "# For off policy learning only: get greedy policy (no exploration)\n",
        "#agent._policy = get_epsilon_greedy_policy_from_action_values(agent._Q.weight)\n",
        "# Run Eval\n",
        "avarge_return, num_episode = utils.evaluate_multiagent(agent_dict, env, 5000)\n",
        "for ag_id, value in avarge_return.items():\n",
        "    print(f\"{ag_id}: Average return = {value / num_episode}\")\n",
        "\n",
        "utils.plot_history(history)\n",
        "utils.plot_history(eps_history, smoothing=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  -  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  X  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  -  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  X  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  -  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  X  |  O  |  -  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  X  |  O  |  O  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  -  |  X  |  O  \n",
            "     |     |     \n",
            "     |     |     \n",
            "  X  |  O  |  O  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  O  |  X  |  X  \n",
            "_____|_____|_____\n",
            "     |     |     \n",
            "  X  |  X  |  O  \n",
            "     |     |     \n"
          ]
        }
      ],
      "source": [
        "env = tictactoe_v3.env(render_mode='human')\n",
        "reward, _ = utils.play_multiagent_episode(agent_dict, env)\n",
        "#html=utils.render_mp4(video_path)\n",
        "#HTML(html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b007964239c9846de49217bea874a76b6e18c6041f326c6a02623c321aae0990"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

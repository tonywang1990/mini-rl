{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tonywy/micromamba/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cpu\n",
            "dqn_agent test passed!\n",
            "using device: cpu\n",
            "a2c_agent test passed!\n",
            "using device: cpu\n",
            "ppo_agent passed!\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Setup Environment\n",
        "import sys\n",
        "import os\n",
        "ROOT = '../../'\n",
        "sys.path.append(ROOT)\n",
        "\n",
        "from pettingzoo.classic import tictactoe_v3\n",
        "import numpy as np\n",
        "import copy\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "from IPython.display import HTML\n",
        "from source.agents.dqn_agent import DQNAgent\n",
        "from source.agents.advantage_actor_critic_agent import A2CAgent\n",
        "from source.agents.ppo_agent import PPOAgent\n",
        "from source.agents.random_agent import RandomAgent\n",
        "from source.utils import utils\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import random\n",
        "from typing import Dict, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "from source.agents.agent import Agent\n",
        "from pettingzoo.utils.env import AECEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3508249237.py, line 79)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn [2], line 79\u001b[0;36m\u001b[0m\n\u001b[0;31m    logging = utils.train_double_agent(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Policy Eval\n",
        "# Params\n",
        "num_epoch = 100\n",
        "num_episode = 1000\n",
        "random_seed = 101\n",
        "\n",
        "# Initialize\n",
        "# set random seeds\n",
        "rng = np.random.default_rng(random_seed)\n",
        "#random.seed(random_seed)\n",
        "#torch.manual_seed(random_seed)\n",
        "# epsilon decay\n",
        "\n",
        "# Create Environment.\n",
        "#env = gym.make('Taxi-v3')\n",
        "# using render_mode=rgb_array so that video recording works\n",
        "#env = gym.make(\n",
        "#    \"LunarLander-v2\",\n",
        "#    render_mode='rgb_array'\n",
        "#)\n",
        "#env = gym.make(\"CarRacing-v2\", render_mode='rgb_array', continuous=False)\n",
        "#env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "\n",
        "env = tictactoe_v3.env()\n",
        "env.np_random = rng\n",
        "\n",
        "# Create Agent\n",
        "## Best performance (dqn, random): 0 loss in 1000, eps_decay=1e7\n",
        "## Best performance (dqn, dqn): 1000 draw in 1000, eps_decay=1e8\n",
        "dqn_agent = DQNAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.7,\n",
        "    epsilon=0.9, # use epsilon_schedule\n",
        "    learning_rate=1e-3,\n",
        "    learning=True,\n",
        "    batch_size = 64,\n",
        "    tau = 0.005,\n",
        "    eps_decay=1e8, #1e7 for (dqn, random), 1e8 for (dqn, dqn)\n",
        "    net_params={'width':16, 'n_hidden':1},\n",
        "    update_freq=1\n",
        ")\n",
        "## Best performance (a2c, random): ~50 loss in 1000, discount=0.5, width=256\n",
        "## \n",
        "a2c_agent = A2CAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.5,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    policy_lr=1e-3,\n",
        "    value_lr=1e-3,\n",
        "    net_params={'width':256, 'n_hidden':1}, #128 perform best with temp = 1\n",
        "    tempreture=1\n",
        ")\n",
        "ppo_agent = PPOAgent(\n",
        "    state_space=env.observation_space('player_1')['observation'],\n",
        "    action_space=env.action_space('player_1'),\n",
        "    discount_rate=0.5,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    policy_lr=3e-4, #spinup default=3e-4\n",
        "    value_lr=1e-3, #spinup default=1e-3\n",
        "    net_params={'width':32, 'n_hidden':1},\n",
        "    gae_lambda=1.0, #spinup default=0.97\n",
        "    clip_ratio=0.2, #spinup default=0.1\n",
        "    num_updates=5, #spinup default=80\n",
        "    entropy_coeff=0.01,\n",
        ")\n",
        "random_agent = RandomAgent(    \n",
        "    state_space=env.observation_space('player_2')['observation'],\n",
        "    action_space=env.action_space('player_2'),\n",
        "    discount_rate=None,\n",
        "    epsilon=None, # use epsilon_schedule\n",
        "    learning_rate=None,\n",
        "    learning=False\n",
        ")\n",
        "logging = utils.train_double_agent(\n",
        "    env=env, \n",
        "    agent_dict={'player_1': ppo_agent, 'player_2': random_agent}, \n",
        "    num_epoch=20, \n",
        "    num_episode=1000, \n",
        "    self_play=False, \n",
        "    shuffle=False, \n",
        "    verbal=True\n",
        ")\n",
        "utils.plot_training_logs(logging)\n",
        "xx\n",
        "\n",
        "#agent_dict = {}\n",
        "#agent_dict['player_1'] = ppo_agent\n",
        "#clone = copy.deepcopy(ppo_agent)\n",
        "#lone._learning = False\n",
        "#agent_dict['player_2'] = clone\n",
        "agent_dict = {'player_1': ppo_agent, 'player_2':random_agent}\n",
        "\n",
        "log_history = []\n",
        "# Start Learning\n",
        "for i in range(num_epoch):\n",
        "    p1_reward, episode_len, policy_loss, value_loss = [], [], [], []\n",
        "    for _ in tqdm(range(num_episode)):\n",
        "        logs = utils.play_multiagent_episode(agent_dict, env, shuffle=False, debug=True)#,epsilon=epsilon_schedule[i])\n",
        "        p1_reward.append(logs['player_1']['reward'])\n",
        "        episode_len.append(logs['player_1']['episode_len'])\n",
        "        policy_loss.append(logs['player_1']['policy_loss'])\n",
        "        value_loss.append(logs['player_1']['value_loss'])\n",
        "        log_history.append(logs)\n",
        "    #agent_dict['player_2'].update_weights_from(agent_dict['player_1'], tau=1)\n",
        "    print(\n",
        "        f\"step: {i},  epsilon:{agent_dict['player_1']._epsilon}, average_return: {np.mean(p1_reward)}, average_epi_len: {np.mean(episode_len)}, p1 win: {p1_reward.count(1)}, lose: {p1_reward.count(-1)}, draw: {p1_reward.count(0)}, policy_loss: {np.mean(policy_loss)}, value_loss: {np.mean(value_loss)}\")\n",
        "\n",
        "utils.plot_logs(log_history)\n",
        "#utils.plot_history(eps_history, smoothing=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = tictactoe_v3.env(render_mode='human')\n",
        "utils.play_multiagent_episode(agent_dict, env, shuffle=False ,debug=False)\n",
        "#html=utils.render_mp4(video_path)\n",
        "#HTML(html)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b007964239c9846de49217bea874a76b6e18c6041f326c6a02623c321aae0990"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
